#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import argparse
import logging
import subprocess


from modules.Utilities import Utilities
from modules.SampleDataParser import SampleDataParser
from modules.RefDataArray import RefDataArray
from modules.RefDataLine import RefDataLine
from modules.PathsKeeper import PathsKeeper
from modules.Aligner import Aligner
from modules.CoverageExtractor import CoverageExtractor


class Initializer:
    def __init__(self):
        namespace = self._parse_args()
        self.sampledata_file_name = namespace.input
        self.refdata_file_name = namespace.refdata
        self.input_mask = namespace.mask
        # *_output_mask are attributes of RefDataLine class
        self.threads_number = self._parse_threads_number(namespace.threads)
        self.no_coverage_bool = namespace.no_coverage
        self.output_dir = Utilities.ends_with_slash(namespace.output)
        self.logs_directory = "{}Logs/".format(self.output_dir)
        [os.makedirs(i, exist_ok=True) for i in [self.output_dir, self.logs_directory]]

    @staticmethod
    def _parse_args():
        starting_parser = argparse.ArgumentParser(description="""
        This script performs single alignment & postprocessing for every colorspace or non-colorspace sequence file specified in the input file list.
        Required software: bowtie, bowtie2, samtools, bedtools. 
        If all coverage extract parameters specified, the script also will look for 'sam2coverage.py' within the same directory""")
        starting_parser.add_argument("-i", "--input", required=True,
                                     help="Input list containing two tab-delimited columns for colorspace or non-colorspace sequences and three for paired-end sequences: sample name and absolute path(s). May contain a header")
        starting_parser.add_argument("-r", "--refdata", default=None,
                                     help="Linker file generated by the 'cook_the_reference.py' script. Contains JSON or tab-delimited columns: original FASTA file name, bowtie colorspace index, bowtie2 index, FASTA samtools index, genome lengths file, annotation")
        starting_parser.add_argument("-m", "--mask", default="",
                                     help="Mask to be added to resulting files containing reference database name, date etc")
        starting_parser.add_argument("-t", "--threads", default="max",
                                     help="(Optional) Number of CPU cores to use. Also may utilize 'max' (default), 'half', 'third' and 'two_thirds' of total available threads")
        starting_parser.add_argument("-n", "--no_coverage", default=False, action='store_true',
                                     help="(Optional) If selected, cancels coverage extraction")
        starting_parser.add_argument("-o", "--output", required=True,
                                     help="Output directory")
        return starting_parser.parse_args()

    @staticmethod
    def _parse_threads_number(s):
        t = int(subprocess.getoutput("nproc").strip())
        o = 0
        if len(s) > 0:
            if s.isnumeric():
                s = int(s)
                if s > 0:
                    o = min(t, s)
                else:
                    o = t
            else:
                if s == "max":
                    o = t
                elif s == "half":
                    o = int(t / 2)
                elif s == "third":
                    o = int(t / 3)
                elif s == "two_thirds":
                    o = int(t * 2 / 3)
        if o == 0:
            print("Cannot parse the threads number: '{s}'. Using threads number: '{t}'".format(s=s, t=t))
            o = t
        return o


class PipelineHandler:
    def __init__(self, refdata):
        self._refdata = refdata

    def _run_aligner(self, sampledata):
        keeper = PathsKeeper(sampledata=sampledata, refdata=self._refdata, output_dir=mainInitializer.output_dir)
        aligner = Aligner(keeper, threads_number=mainInitializer.threads_number)
        aligner.run()

    def _run_extractor(self, sampledata):
        keeper = PathsKeeper(sampledata=sampledata, refdata=self._refdata, output_dir=mainInitializer.output_dir)
        extractor = CoverageExtractor(keeper)
        extractor.run()

    def run(self):
        Utilities.single_core_queue(func=self._run_aligner, queue=sampleFilesList)
        if not mainInitializer.no_coverage_bool:
            Utilities.multi_core_queue(func=self._run_extractor, queue=sampleFilesList, processes=mainInitializer.threads_number)


class ChunksHandler:
    def __init__(self):
        self.chunks_list = RefDataArray.read(mainInitializer.refdata_file_name).get_parsed_list()

    @staticmethod
    def _run_pipeline(refdata: RefDataLine):
        handler = PipelineHandler(refdata)
        handler.run()

    def run(self):
        Utilities.single_core_queue(func=self._run_pipeline, queue=self.chunks_list)


if __name__ == '__main__':
    mainInitializer = Initializer()
    launchTime = Utilities.get_time()
    nodeName = subprocess.getoutput("hostname").strip()
    mainLogFile = "{a}nBee_{b}_{c}.log".format(a=mainInitializer.logs_directory, b=nodeName, c=launchTime)
    print("Started main workflow with log file: '{}'".format(mainLogFile))
    logging.basicConfig(format=u'%(levelname)-8s [%(asctime)s] %(message)s',
                        level=logging.DEBUG,
                        handlers=[logging.FileHandler(mainLogFile), logging.StreamHandler()])
    sampleDataParser = SampleDataParser(mainInitializer.sampledata_file_name)
    sampleFilesList = sampleDataParser.get_parsed_list()
    if len(sampleFilesList) == 0:
        Utilities.log_and_raise("No files to process: {}".format(sampleFilesList))
    chunksHandler = ChunksHandler()
    chunksHandler.run()
